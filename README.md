# Task Processing System

This is a microservice-based task processing system built with Python, FastAPI, Celery, RabbitMQ, PostgreSQL, and Docker. The system processes tasks asynchronously using background jobs managed by Celery, with task metadata stored in a PostgreSQL database. RabbitMQ serves as the message broker to handle task queuing and distribution.

## System Overview

1. **API Service**  
   - Exposes CRUD operations for tasks.
   - Sends jobs to RabbitMQ for background processing.
  
2. **Worker Service**  
   - Listens for jobs from RabbitMQ.
   - Processes tasks asynchronously using Celery.

3. **RabbitMQ**  
   - Message broker for task distribution and queuing.

4. **PostgreSQL**  
   - Stores task metadata, including task status and details.

## Folder Structure
```markdown
message_queue/
â”‚â”€â”€ api/
â”‚   â”œâ”€â”€ main.py   # FastAPI app with CRUD endpoints
â”‚   â”œâ”€â”€ db.py     # Database connection setup
â”‚   â”œâ”€â”€ models.py # SQLAlchemy models for tasks
â”‚   â”œâ”€â”€ schemas.py # Pydantic schemas for validation
â”‚   â”œâ”€â”€ worker.py # Celery worker to process background jobs
â”‚â”€â”€ docker-compose.yml  # Docker Compose configuration
â”‚â”€â”€ Dockerfile.api      # Dockerfile for API service
â”‚â”€â”€ Dockerfile.worker   # Dockerfile for worker service
â”‚â”€â”€ requirements.txt    # Python dependencies
```

## How to Run

### Prerequisites

- **Docker** and **Docker Compose** must be installed on your system.

### 1. Start all services

Navigate to the project directory and use Docker Compose to build and start the services:

```bash
docker-compose up --build
```

2. Access RabbitMQ UI
	â€¢	Open RabbitMQ Management UI in your browser: http://localhost:15672/
	â€¢	Login with username: guest and password: guest

3. Test API Endpoints

You can interact with the API using curl or any API client like Postman.

Create a new task:

```bash
curl -X POST "http://localhost:8000/tasks/" -H "Content-Type: application/json" -d '{"name": "Process Data"}'
```

Get task status:
```bash
curl -X GET "http://localhost:8000/tasks/1"
```

ðŸ“Œ Key Features
	â€¢	CRUD API: Create and retrieve tasks.
	â€¢	Message Queue (RabbitMQ): Ensures tasks are processed asynchronously.
	â€¢	Job Scheduler (Celery): Executes background tasks.
	â€¢	Microservices Architecture: API and worker services run independently.
	â€¢	Dockerized: Easy deployment with docker-compose up.


## Additional feature.

### 1. Using DBeaver (Cross-Platform GUI)

DBeaver is a free, open-source database management tool.

Step 1: Install DBeaver

Download from [Download DBeaver](https://dbeaver.io/download/).

Step 2: Connect to PostgreSQL
```markdown
	â€¢	Open DBeaver.
	â€¢	Click New Database Connection â†’ Select PostgreSQL.
	â€¢	Enter:
	â€¢	Host: localhost (or host.docker.internal for macOS/Windows)
	â€¢	Port: 5432
	â€¢	Database: Your database name
	â€¢	Username: postgres
	â€¢	Password: Your password
	â€¢	Click Test Connection, then Finish.
```

Step 3: View the task table in the DBeaver

![dbeaver](image.png)

Step 4: Start creating a new task with postman and results are visible in the task table.
